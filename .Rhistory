colnames(tdm) = emos
#require(wordcloud)
suppressWarnings(comparison.cloud(tdm, colors = brewer.pal(n_emos, "Dark2"), scale = c(3,.5), random.order = FALSE, title.size = 1.5))
}
getWordCloud(gojekSentimenDF_en, tgojekenCleaned, gojekEmotion_en)
getWordCloud(grabSentimenDF_en, tgrabenCleaned, grabEmotion_en)
#export ke csv jika sudah ada tidak perlu lagi
#write.csv(gojekSentimenDF_en, file = "gojeksentimen_en.csv")
#write.csv(grabSentimenDF_en, file = "grabsentimen_en.csv")
library(tools, lib.loc = "C:/Program Files/R/R-3.6.2/library")
library(twitteR)
library(ROAuth)
api_key<- "Fd9PATDCguKYADpvqBqrzbZCA"
api_secret<- "fSM942cLTOWd8mK3l0o7SJ2j93b1Zmq5zAHomX6LbaJmP0U8kU"
access_token<- "401888503-50qwjZIOnPtHEC6ihy7msHMiAfuS2S1udwtzBUEB"
access_token_secret<- "lzlYQRFWrClOF8zo31Gve466J7w5TNMBiMHs2AOUyzUB9"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
gojek_tweets_en = searchTwitter("gojek", n=1000, lang="en")
grab_tweets_en = searchTwitter("grab", n=1000, lang="en")
gojek_tweets_id = searchTwitter("gojek", n=1000, lang="id")
grab_tweets_id = searchTwitter("grab", n=1000, lang="id")
#cleaning text from meta informations, URLs, #hashtags, punctuation marks, numbers, unnecessary spaces, retweets (RTs)
#gunakan code blocks berikut:
gojekTweetsid <- sapply(gojek_tweets_id, function(x) x$getText())
grabTweetsid <- sapply(grab_tweets_id, function(x) x$getText())
gojekTweetsen <- sapply(gojek_tweets_en, function(x) x$getText())
grabTweetsen <- sapply(grab_tweets_en, function(x) x$getText())
catch.error = function(x)
{
#buat missing value untuk tujuan tes
y = NA
#test untuk mengecek error (NA) yang telah kita dibuat
catch_error = tryCatch(tolower(x), error=function(e) e)
#if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
#check result if error exists, otherwise the function works fine
return(y)
}
cleanTweets <- function(tweet) {
#bersihkan tweet untuk sentiment analysis
#remove html links:
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
#remove retweet entities:
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
#remove #hashtags:
tweet = gsub("#\\w+", " ", tweet)
#remove all "@people":
tweet = gsub("@\\w+", " ", tweet)
#remove all punctuations:
tweet = gsub("[[:punct:]]", " ", tweet)
#remove numbers, kita hanya butuh teks untuk analytics
tweet = gsub("[[:digit:]]", " ", tweet)
#remove unnecessary spaces (white spaces, tabs, etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
#remove amp
tweet = gsub("&amp", " ", tweet)
#remove crazy character
tweet = gsub("[^a-zA-Z0-9]", " ", tweet)
#remove alphanumeric
tweet = gsub("[^[:alnum:]]", " ", tweet)
#jika ada lagi yang dirasa ingin dihilangkan, bisa. Contohnya, slang words/bahasa gaul dapat dihilangkan dengan cara serupa di atas.
#ubah semua kata menjadi lowercase:
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs <- function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
#remove "NA" tweets:
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
#remove repetitive tweets:
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
#eksekusi pembersihan tweet
tgojekidCleaned = cleanTweetsAndRemoveNAs(gojekTweetsid)
tgrabidCleaned = cleanTweetsAndRemoveNAs(grabTweetsid)
tgojekenCleaned = cleanTweetsAndRemoveNAs(gojekTweetsen)
tgrabenCleaned = cleanTweetsAndRemoveNAs(grabTweetsen)
#export data ke csv
write.csv(tgojekidCleaned, file = "tgojekidCleaned.csv")
write.csv(tgrabidCleaned, file = "tgrabidCleaned.csv")
write.csv(tgojekenCleaned, file = "tgojekenCleaned.csv")
write.csv(tgrabenCleaned, file = "tgrabenCleaned.csv")
#import lexicon
opinion.lexicon.posid = scan("positive_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.negid = scan("negative_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.posen = scan("positive_words_en.txt", what = "character", comment.char = ";")
opinion.lexicon.negen = scan("negative_words_en.txt", what = "character", comment.char = ";")
posid.words = c(opinion.lexicon.posid)
negid.words = c(opinion.lexicon.negid)
posen.words = c(opinion.lexicon.posen)
negen.words = c(opinion.lexicon.negen)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreid = function(sentences, posid.words, negid.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posid.words, negid.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
#sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posid.matches = !is.na(match(words, posid.words))
negid.matches = !is.na(match(words, negid.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posid.matches) - sum(negid.matches)
return(score)
}, posid.words, negid.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekidResult = getSentimentScoreid(tgojekidCleaned, posid.words, negid.words)
grabidResult = getSentimentScoreid(tgrabidCleaned, posid.words, negid.words)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreen = function(sentences, posen.words, negen.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posen.words, negen.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posen.matches = !is.na(match(words, posen.words))
negen.matches = !is.na(match(words, negen.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posen.matches) - sum(negen.matches)
return(score)
}, posen.words, negen.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekenResult = getSentimentScoreen(tgojekenCleaned, posen.words, negen.words)
grabenResult = getSentimentScoreen(tgrabenCleaned, posen.words, negen.words)
#export ke csv:
write.csv(gojekidResult, file = "gojekResultid.csv")
write.csv(grabidResult, file = "grabResultid.csv")
write.csv(gojekenResult, file = "gojekResulten.csv")
write.csv(grabenResult, file = "grabResulten.csv")
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
write.csv(grabenResult, file = "grabResulten.csv")
library(twitteR)
library(ROAuth)
api_key<- "Fd9PATDCguKYADpvqBqrzbZCA"
api_secret<- "fSM942cLTOWd8mK3l0o7SJ2j93b1Zmq5zAHomX6LbaJmP0U8kU"
access_token<- "401888503-50qwjZIOnPtHEC6ihy7msHMiAfuS2S1udwtzBUEB"
access_token_secret<- "lzlYQRFWrClOF8zo31Gve466J7w5TNMBiMHs2AOUyzUB9"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
gojek_tweets_en = searchTwitter("gojek", n=1000, lang="en")
grab_tweets_en = searchTwitter("grab", n=1000, lang="en")
gojek_tweets_id = searchTwitter("gojek", n=1000, lang="id")
grab_tweets_id = searchTwitter("grab", n=1000, lang="id")
#cleaning text from meta informations, URLs, #hashtags, punctuation marks, numbers, unnecessary spaces, retweets (RTs)
#gunakan code blocks berikut:
gojekTweetsid <- sapply(gojek_tweets_id, function(x) x$getText())
grabTweetsid <- sapply(grab_tweets_id, function(x) x$getText())
gojekTweetsen <- sapply(gojek_tweets_en, function(x) x$getText())
grabTweetsen <- sapply(grab_tweets_en, function(x) x$getText())
catch.error = function(x)
{
#buat missing value untuk tujuan tes
y = NA
#test untuk mengecek error (NA) yang telah kita dibuat
catch_error = tryCatch(tolower(x), error=function(e) e)
#if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
#check result if error exists, otherwise the function works fine
return(y)
}
cleanTweets <- function(tweet) {
#bersihkan tweet untuk sentiment analysis
#remove html links:
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
#remove retweet entities:
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
#remove #hashtags:
tweet = gsub("#\\w+", " ", tweet)
#remove all "@people":
tweet = gsub("@\\w+", " ", tweet)
#remove all punctuations:
tweet = gsub("[[:punct:]]", " ", tweet)
#remove numbers, kita hanya butuh teks untuk analytics
tweet = gsub("[[:digit:]]", " ", tweet)
#remove unnecessary spaces (white spaces, tabs, etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
#remove amp
tweet = gsub("&amp", " ", tweet)
#remove crazy character
tweet = gsub("[^a-zA-Z0-9]", " ", tweet)
#remove alphanumeric
tweet = gsub("[^[:alnum:]]", " ", tweet)
#jika ada lagi yang dirasa ingin dihilangkan, bisa. Contohnya, slang words/bahasa gaul dapat dihilangkan dengan cara serupa di atas.
#ubah semua kata menjadi lowercase:
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs <- function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
#remove "NA" tweets:
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
#remove repetitive tweets:
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
#eksekusi pembersihan tweet
tgojekidCleaned = cleanTweetsAndRemoveNAs(gojekTweetsid)
tgrabidCleaned = cleanTweetsAndRemoveNAs(grabTweetsid)
tgojekenCleaned = cleanTweetsAndRemoveNAs(gojekTweetsen)
tgrabenCleaned = cleanTweetsAndRemoveNAs(grabTweetsen)
#export data ke csv
write.csv(tgojekidCleaned, file = "tgojekidCleaned.csv")
write.csv(tgrabidCleaned, file = "tgrabidCleaned.csv")
write.csv(tgojekenCleaned, file = "tgojekenCleaned.csv")
write.csv(tgrabenCleaned, file = "tgrabenCleaned.csv")
#import lexicon
opinion.lexicon.posid = scan("positive_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.negid = scan("negative_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.posen = scan("positive_words_en.txt", what = "character", comment.char = ";")
opinion.lexicon.negen = scan("negative_words_en.txt", what = "character", comment.char = ";")
posid.words = c(opinion.lexicon.posid)
negid.words = c(opinion.lexicon.negid)
posen.words = c(opinion.lexicon.posen)
negen.words = c(opinion.lexicon.negen)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreid = function(sentences, posid.words, negid.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posid.words, negid.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
#sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posid.matches = !is.na(match(words, posid.words))
negid.matches = !is.na(match(words, negid.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posid.matches) - sum(negid.matches)
return(score)
}, posid.words, negid.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekidResult = getSentimentScoreid(tgojekidCleaned, posid.words, negid.words)
grabidResult = getSentimentScoreid(tgrabidCleaned, posid.words, negid.words)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreen = function(sentences, posen.words, negen.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posen.words, negen.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posen.matches = !is.na(match(words, posen.words))
negen.matches = !is.na(match(words, negen.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posen.matches) - sum(negen.matches)
return(score)
}, posen.words, negen.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekenResult = getSentimentScoreen(tgojekenCleaned, posen.words, negen.words)
grabenResult = getSentimentScoreen(tgrabenCleaned, posen.words, negen.words)
#export ke csv:
write.csv(gojekidResult, file = "gojekResultid.csv")
write.csv(grabidResult, file = "grabResultid.csv")
write.csv(gojekenResult, file = "gojekResulten.csv")
write.csv(grabenResult, file = "grabResulten.csv")
library(twitteR)
library(ROAuth)
library(twitteR)
library(ROAuth)
api_key<- "Fd9PATDCguKYADpvqBqrzbZCA"
api_secret<- "fSM942cLTOWd8mK3l0o7SJ2j93b1Zmq5zAHomX6LbaJmP0U8kU"
access_token<- "401888503-50qwjZIOnPtHEC6ihy7msHMiAfuS2S1udwtzBUEB"
access_token_secret<- "lzlYQRFWrClOF8zo31Gve466J7w5TNMBiMHs2AOUyzUB9"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
gojek_tweets_en = searchTwitter("gojek", n=1000, lang="en")
grab_tweets_en = searchTwitter("grab", n=1000, lang="en")
gojek_tweets_id = searchTwitter("gojek", n=1000, lang="id")
grab_tweets_id = searchTwitter("grab", n=1000, lang="id")
#cleaning text from meta informations, URLs, #hashtags, punctuation marks, numbers, unnecessary spaces, retweets (RTs)
#gunakan code blocks berikut:
gojekTweetsid <- sapply(gojek_tweets_id, function(x) x$getText())
grabTweetsid <- sapply(grab_tweets_id, function(x) x$getText())
gojekTweetsen <- sapply(gojek_tweets_en, function(x) x$getText())
grabTweetsen <- sapply(grab_tweets_en, function(x) x$getText())
catch.error = function(x)
{
#buat missing value untuk tujuan tes
y = NA
#test untuk mengecek error (NA) yang telah kita dibuat
catch_error = tryCatch(tolower(x), error=function(e) e)
#if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
#check result if error exists, otherwise the function works fine
return(y)
}
cleanTweets <- function(tweet) {
#bersihkan tweet untuk sentiment analysis
#remove html links:
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
#remove retweet entities:
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
#remove #hashtags:
tweet = gsub("#\\w+", " ", tweet)
#remove all "@people":
tweet = gsub("@\\w+", " ", tweet)
#remove all punctuations:
tweet = gsub("[[:punct:]]", " ", tweet)
#remove numbers, kita hanya butuh teks untuk analytics
tweet = gsub("[[:digit:]]", " ", tweet)
#remove unnecessary spaces (white spaces, tabs, etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
#remove amp
tweet = gsub("&amp", " ", tweet)
#remove crazy character
tweet = gsub("[^a-zA-Z0-9]", " ", tweet)
#remove alphanumeric
tweet = gsub("[^[:alnum:]]", " ", tweet)
#jika ada lagi yang dirasa ingin dihilangkan, bisa. Contohnya, slang words/bahasa gaul dapat dihilangkan dengan cara serupa di atas.
#ubah semua kata menjadi lowercase:
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs <- function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
#remove "NA" tweets:
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
#remove repetitive tweets:
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
#eksekusi pembersihan tweet
tgojekidCleaned = cleanTweetsAndRemoveNAs(gojekTweetsid)
tgrabidCleaned = cleanTweetsAndRemoveNAs(grabTweetsid)
tgojekenCleaned = cleanTweetsAndRemoveNAs(gojekTweetsen)
tgrabenCleaned = cleanTweetsAndRemoveNAs(grabTweetsen)
#export data ke csv
write.csv(tgojekidCleaned, file = "tgojekidCleaned.csv")
write.csv(tgrabidCleaned, file = "tgrabidCleaned.csv")
write.csv(tgojekenCleaned, file = "tgojekenCleaned.csv")
write.csv(tgrabenCleaned, file = "tgrabenCleaned.csv")
#import lexicon
opinion.lexicon.posid = scan("positive_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.negid = scan("negative_words_id.txt", what = "character", comment.char = ";")
opinion.lexicon.posen = scan("positive_words_en.txt", what = "character", comment.char = ";")
opinion.lexicon.negen = scan("negative_words_en.txt", what = "character", comment.char = ";")
posid.words = c(opinion.lexicon.posid)
negid.words = c(opinion.lexicon.negid)
posen.words = c(opinion.lexicon.posen)
negen.words = c(opinion.lexicon.negen)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreid = function(sentences, posid.words, negid.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posid.words, negid.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
#sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posid.matches = !is.na(match(words, posid.words))
negid.matches = !is.na(match(words, negid.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posid.matches) - sum(negid.matches)
return(score)
}, posid.words, negid.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekidResult = getSentimentScoreid(tgojekidCleaned, posid.words, negid.words)
grabidResult = getSentimentScoreid(tgrabidCleaned, posid.words, negid.words)
#membuat fungsi score.sentiment(), yang bisa menghitung hasil sentimen mentah berdasarkan algoritma pencocokan sederhana:
getSentimentScoreen = function(sentences, posen.words, negen.words, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, posen.words, negen.words) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
posen.matches = !is.na(match(words, posen.words))
negen.matches = !is.na(match(words, negen.words))
#score sentimen = total positive sentiment - total negative:
score = sum(posen.matches) - sum(negen.matches)
return(score)
}, posen.words, negen.words, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
gojekenResult = getSentimentScoreen(tgojekenCleaned, posen.words, negen.words)
grabenResult = getSentimentScoreen(tgrabenCleaned, posen.words, negen.words)
#export ke csv:
write.csv(gojekidResult, file = "gojekResultid.csv")
write.csv(grabidResult, file = "grabResultid.csv")
write.csv(gojekenResult, file = "gojekResulten.csv")
write.csv(grabenResult, file = "grabResulten.csv")
knitr::opts_chunk$set(echo = TRUE)
shinyApp(ui = ui, server = server)
library(dplyr)
library(vroom)
library(here)
library(tidyverse)
library(ggplot2)
library(plotly)
library(tidytext)
library(wordcloud)
library(reshape2)
library(shiny)
# Source data from https://www.kaggle.com/sid321axn/amazon-alexa-reviews
alexa = vroom(here("amazon_alexa.tsv"), delim = "\t")
library(dplyr)
library(vroom)
library(here)
library(tidyverse)
library(ggplot2)
library(plotly)
library(tidytext)
library(wordcloud)
library(reshape2)
library(shiny)
# Source data from https://www.kaggle.com/sid321axn/amazon-alexa-reviews
alexa = vroom(here("amazon_alexa.tsv"), delim = "\t")
Sys.which("make")
install.packages('Rstem', repos = "http://www.omegahat.net/R")
install.packages("rstem")
install.packages("Rstem", repos="http://www.omegahat.net/R", type="source")
Sys.which("make")
install.packages("Rstem", repos="http://www.omegahat.net/R", type="source")
install.packages("C:/Users/D.VA/Downloads/Compressed/sentiment_0.2.tar.gz", repos = NULL, type = "source")
library(SnowballC)
library(devtools)
library(RCurl)
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
install.packages("Rstem_0.4-1.tar.gz" , repos=NULL, type="source")
install.packages("C:/Users/D.VA/Downloads/Compressed/Rstem_0.4-1.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/D.VA/Downloads/Compressed/Rstem_0.4-1.tar.gz", repos = NULL, type = "source")
install.Rtools()
install_url('http://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz')
sessionInfo()
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(twitteR)
library(ROAuth)
library(RCurl)
library(DT)
library(here)
library(ggplot2)
library(plotly)
library(tidyverse)
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
library(shiny); runApp('C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001/sentimentAnalysis.R')
setwd("C:/Users/D.VA/Downloads/drive-download-20200531T111718Z-001")
library(shiny); runApp('sentimentAnalysis.R')
library(shiny); runApp('sentimentAnalysis.R')
